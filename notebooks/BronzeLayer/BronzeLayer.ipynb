{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba45390f-7bab-4ec8-9b64-3b9eaf879bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.widgets.text(\"src\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcb8e640-4d86-45d9-8ee0-d3067dd5b917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# src_value = dbutils.widgets.get(\"src\")\n",
    "# src_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5944e059-0296-43b7-a9b2-c8961412d224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Create a streaming DataFrame using Databricks Auto Loader (cloudFiles)\n",
    "# df = (\n",
    "#     spark.readStream.format(\"cloudFiles\")\n",
    "#     # Specify the format of incoming files (CSV in this case)\n",
    "#     .option(\"cloudFiles.format\", \"csv\")\n",
    "#     # Location to store schema and checkpoint info\n",
    "#     # - Tracks which files are already processed (avoids duplicates)\n",
    "#     # - Stores inferred schema so it's not recomputed each time\n",
    "#     .option(\"cloudFiles.schemaLocation\", f\"/Volumes/workspace/flight_bronze/bronzevolume/{src_value}/checkpoint\")\n",
    "#     # Handle schema evolution:\n",
    "#     # \"rescue\" means unexpected new columns are captured in \"_rescued_data\" instead of failing\n",
    "#     .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "#     # Path where Auto Loader will look for new incoming CSV files\n",
    "#     # Uses src_value variable so each source has its own folder\n",
    "#     .load(f\"/Volumes/workspace/flight_raw/rawvolume/rawdata/{src_value}/\")\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82252033-812a-4d52-97b9-3a643ce40880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Write the streaming DataFrame into a Delta table (Bronze layer)\n",
    "# (\n",
    "#     df.writeStream.format(\"delta\")\n",
    "#     # Append mode: new data gets added without modifying existing records\n",
    "#     .outputMode(\"append\")\n",
    "#     # Trigger set to \"once\":\n",
    "#     .trigger(once=True)\n",
    "#     # Checkpoint location for fault tolerance\n",
    "#     .option(\"checkpointLocation\", f\"/Volumes/workspace/flight_bronze/bronzevolume/{src_value}/checkpoint\")\n",
    "#     # Final storage path for the Delta table\n",
    "#     .option(\"path\", f\"/Volumes/workspace/flight_bronze/bronzevolume/{src_value}/data\")\n",
    "#     # Start the streaming write query\n",
    "#     .start()\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a128967a-871d-4929-9872-cf179393eac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# SELECT * FROM delta.`/Volumes/workspace/flight_bronze/bronzevolume/customers/data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dce1a66c-7174-478d-a104-5d0d6de42246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# -- Make sure schema exists\n",
    "# CREATE SCHEMA IF NOT EXISTS workspace.flight_raw;\n",
    "# CREATE SCHEMA IF NOT EXISTS workspace.flight_bronze;\n",
    "\n",
    "# -- Register the Volumes (this tells UC about the folders you already have)\n",
    "# CREATE VOLUME IF NOT EXISTS workspace.flight_raw.raw_volume;\n",
    "# CREATE VOLUME IF NOT EXISTS workspace.flight_bronze.bronzevolume;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cadcb1cd-c762-4ad2-9626-72dc25447e7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# SHOW VOLUMES IN workspace.flight_raw;\n",
    "# SHOW VOLUMES IN workspace.flight_bronze;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce53fbc2-cbcb-4b3e-9f6c-498fb22ca0c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "\n",
    "\n",
    "yaml_path = \"/Workspace/Users/tushhaar24@gmail.com/Databricks_flight_project/Databricks_Flight_Data_Project/configs/autoloader_bronze.yml\"\n",
    "\n",
    "with open(yaml_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "autoloader_cfg = config[\"autoloader\"]\n",
    "\n",
    "\n",
    "file_format = autoloader_cfg[\"file_format\"]\n",
    "options = autoloader_cfg.get(\"options\", {})\n",
    "checkpoint_template = autoloader_cfg[\"checkpoint_location\"]\n",
    "target_template = autoloader_cfg[\"target_path\"]\n",
    "source_template = autoloader_cfg[\"source_path\"]\n",
    "tables = autoloader_cfg[\"tables\"]\n",
    "\n",
    "\n",
    "dbutils.widgets.text(\"src\", \"\")\n",
    "src_value = dbutils.widgets.get(\"src\")\n",
    "\n",
    "\n",
    "if src_value and src_value in tables:\n",
    "    tables_to_process = [src_value]\n",
    "else:\n",
    "    tables_to_process = tables\n",
    "\n",
    "print(f\"Tables selected for ingestion: {tables_to_process}\")\n",
    "\n",
    "\n",
    "for table in tables_to_process:\n",
    "    print(f\"Processing table: {table}\")\n",
    "\n",
    "    \n",
    "    checkpoint_path = checkpoint_template.format(table=table)\n",
    "    target_path = target_template.format(table=table)\n",
    "    source_path = source_template.format(table=table)\n",
    "\n",
    "    \n",
    "    df = (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", file_format)\n",
    "        .options(**options)  \n",
    "        .option(\"cloudFiles.schemaLocation\", checkpoint_path)\n",
    "        .load(source_path)\n",
    "    )\n",
    "\n",
    "    \n",
    "    (\n",
    "        df.writeStream.format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .trigger(once=True)\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        .option(\"path\", target_path)\n",
    "        .start()\n",
    "    )\n",
    "\n",
    "print(\"Bronze ingestion job submitted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca2fb919-1c27-444b-bb32-c5bf2f279078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7663220220168752,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BronzeLayer",
   "widgets": {
    "src": {
     "currentValue": "bookings",
     "nuid": "5f08c458-d371-4189-84e1-4af15fd23d01",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "src",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "src",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
